---
layout: default
title: Publications
---

<div id="home">
  <h2><i class="fa fa-bookmark"></i> Publications</h2>

<a href="https://arxiv.org/abs/2407.01102">BERGEN: A Benchmarking Library for Retrieval-Augmented Generation
</a><br>
  David Rau, Hervé Déjean, <u>Nadezhda Chirkova</u>, Thibault Formal, Shuai Wang, Vassilina Nikoulina, Stéphane Clinchant<br>
  Findings of EMNLP 2024 
  <br><a href="https://github.com/naver/bergen">[code]</a>

  <br><br>
<a href="https://arxiv.org/abs/2402.14778">Retrieval-augmented generation in multilingual settings
</a><br>
  <u>Nadezhda Chirkova</u>, David Rau, Hervé Déjean, Thibault Formal, Stéphane Clinchant, Vassilina Nikoulina<br>
  Knowledgeable LLMs workshop at ACL 2024
  <br><a href="https://github.com/naver/bergen/blob/main/documentations/multilingual.md">[code]</a>

  <br><br>
  <a href="https://arxiv.org/abs/2402.14778">Zero-shot cross-lingual transfer in instruction tuning of large language models
</a><br>
  <u>Nadezhda Chirkova</u>, Vassilina Nikoulina<br>
  INLG 2024
  <br><a href="https://github.com/naver/pasero/tree/main/examples/zero-shot-transfer-inst-tuning">[code]</a>

 <br><br>
<a href="https://arxiv.org/abs/2402.12279">Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks
</a><br>
  <u>Nadezhda Chirkova</u>, Vassilina Nikoulina<br>
  NAACL 2024

  <br><br>
 <a href="https://arxiv.org/abs/2306.17757">Should you marginalize over possible tokenizations?</a><br>
  <u>Nadezhda Chirkova</u>, Germán Kruszewski, Jos Rozen, Marc Dymetman<br>
  ACL 2023
  <br><a href="https://github.com/naver/marginalization">[code]</a>
  

  <br><br>
   <a href="https://arxiv.org/abs/2308.00683">CodeBPE: investigating subtokenization options for large language model pretraining on source code</a><br>
  <u>Nadezhda Chirkova</u>, Sergey Troshin<br>
  ICLR 2023, and also DL4Code workshop at ICLR 2022 (spotlight)

  <br><br>
   <a href="https://arxiv.org/abs/2202.08975">Probing pretrained models of source code</a><br>
  Sergey Troshin, <u>Nadezhda Chirkova</u><br>
  BlackBox NLP Workshop at EMNLP 2022
  <br><a href="https://github.com/serjtroshin/probings4code">[code]</a>

  <br><br>
   <a href="https://arxiv.org/abs/2212.05901">Parameter-efficient finetuning of Transformers for source code</a><br>
  Shamil Ayupov, <u>Nadezhda Chirkova</u><br>
  Workshop on Efficient Natural Language Processing at NeurIPS 2022
  
  <br><br>
  <a href="https://arxiv.org/abs/2106.15739">On the periodic behavior of neural network training with batch normalization and weight decay</a><br>
  Ekaterina Lobacheva*, Maxim Kodryan*, <u>Nadezhda Chirkova</u>, Andrey Malinin, Dmitry Vetrov<br>
  NeurIPS 2021<br>
  <a href="https://github.com/tipt0p/periodic_behavior_bn_wd">[code]</a>
  
  <br><br>
  <a href="https://arxiv.org/abs/2010.07987">Empirical study of Transformers for source code</a><br>
  <u>Nadezhda Chirkova</u>, Sergey Troshin<br>
  ESEC/FSE 2021<br>
  <a href="https://github.com/bayesgroup/code_transformers">[code]</a>
  <a href="https://youtu.be/cikBL7mgveI">[video]</a>
  
  <br><br>
  <a href="https://arxiv.org/abs/2010.12663">
    A simple approach for handling out-of-vocabulary identifiers in deep learning for source code</a><br>
  <u>Nadezhda Chirkova</u>*, Sergey Troshin*<br>
  NAACL 2021<br>
  <a href="https://github.com/bayesgroup/code_transformers">[code]</a>
   <a href="https://youtu.be/-EPzNDglP2g">[video]</a>
  
  <br><br>
  <a href="https://arxiv.org/abs/2010.12693">On the embeddings of variables in recurrent neural networks for source code</a><br>
  <u>Nadezhda Chirkova</u><br>
  NAACL 2021<br>
  <a href="https://github.com/nadiinchi/dynamic_embeddings">[code]</a>
  
  <br><br>
  <a href="https://arxiv.org/abs/2007.08483">On power laws in deep ensembles</a><br>
  Ekaterina Lobacheva, <u>Nadezhda Chirkova</u>, Maxim Kodryan, Dmitry Vetrov<br>
  NeurIPS 2020 (spotlight)<br>
  <a href="https://github.com/nadiinchi/power_laws_deep_ensembles">[code]</a>
  <a href="https://youtu.be/Z2Aj_jxONQo">[video]</a>
  
  <br><br>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5938">Structured sparsification of gated recurrent neural networks</a><br>
  Ekaterina Lobacheva*, <u>Nadezhda Chirkova*</u>, Alexander Markovich, Dmitry Vetrov<br>
  AAAI 2020<br>
  <a href="https://github.com/tipt0p/SparseBayesianRNN">[code]</a>
  
  <br><br>
  <a href="https://arxiv.org/abs/1810.10927">Bayesian compression for natural language processing</a><br>
  <u>Nadezhda Chirkova</u>, Ekaterina Lobacheva, Dmitry Vetrov<br>
  EMNLP 2018<br>
  <a href="https://github.com/tipt0p/SparseBayesianRNN">[code]</a>
  
</div>

